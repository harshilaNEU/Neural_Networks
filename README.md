# HW to Chapters 6  “Deep Neural Networks” and 7 "Activation Functions"

## Name : Harshila Jagtap 

NEU ID : 002743674 

## Question :

1. Develop linear, ReLU, sigmoid, tanh, and softmax activation functions as a class for neural networks implementation.
2. Develop the class structure and forward propagation including the loss (cost) function implementation for a deep (multilayer) neural network.
3. Develop the backpropagation implementation for a deep (multilayer) neural network.

## Pre-requisite :

1. Visual Studio Code
2. Python

## Sample Output :
![Output](https://github.com/harshilaNEU/Neural_Networks/blob/HW_6_7_Activation_Function/Reference_Images/output.png)

