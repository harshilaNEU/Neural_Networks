# HW to Chapters 6  “Deep Neural Networks” and 7 "Activation Functions"

## Name : Harshila Jagtap 

NEU ID : 002743674 

## Question :

1. Develop linear, ReLU, sigmoid, tanh, and softmax activation functions as a class for neural networks implementation.
2. Develop the class structure and forward propagation including the loss (cost) function implementation for a deep (multilayer) neural network.
3. Develop the backpropagation implementation for a deep (multilayer) neural network.

## Pre-requisite :

1. Visual Studio Code
2. Python

## Scenarios validated with necessary outputs using code


## Sample Output :



