{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss (Cost) after iteration 0: 0.3244606768759905\n",
      "Loss (Cost) after iteration 100: 0.3438764737302939\n",
      "Loss (Cost) after iteration 200: 0.3511308043820638\n",
      "Loss (Cost) after iteration 300: 0.3537922151415003\n",
      "Loss (Cost) after iteration 400: 0.3547572249820521\n",
      "Loss (Cost) after iteration 500: 0.35509986163894747\n",
      "Loss (Cost) after iteration 600: 0.35520347747646647\n",
      "Loss (Cost) after iteration 700: 0.35520450252153285\n",
      "Loss (Cost) after iteration 800: 0.35515274359102694\n",
      "Loss (Cost) after iteration 900: 0.3550455316888044\n",
      "Loss (Cost) after iteration 1000: 0.3548025387564004\n",
      "Loss (Cost) after iteration 1100: 0.35423871212845504\n",
      "Loss (Cost) after iteration 1200: 0.35231277300213726\n",
      "Loss (Cost) after iteration 1300: 0.3425106879158974\n",
      "Loss (Cost) after iteration 1400: 0.26831529795365305\n",
      "Loss (Cost) after iteration 1500: 0.03292301060655237\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/36/chw789xx1jv9xm2cl1v87_4c0000gn/T/ipykernel_26062/2826257561.py:80: RuntimeWarning: divide by zero encountered in divide\n",
      "  dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
      "/var/folders/36/chw789xx1jv9xm2cl1v87_4c0000gn/T/ipykernel_26062/2826257561.py:80: RuntimeWarning: invalid value encountered in divide\n",
      "  dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
      "/var/folders/36/chw789xx1jv9xm2cl1v87_4c0000gn/T/ipykernel_26062/2826257561.py:93: RuntimeWarning: invalid value encountered in multiply\n",
      "  dZ = np.dot(next_layer_W.T, next_layer_dZ) * current_layer.activation.derivative(Z)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss (Cost) after iteration 1600: nan\n",
      "Loss (Cost) after iteration 1700: nan\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Activation:\n",
    "    def __init__(self, activation_type):\n",
    "        self.type = activation_type\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.type == 'linear':\n",
    "            return x\n",
    "        elif self.type == 'relu':\n",
    "            return np.maximum(0, x)\n",
    "        elif self.type == 'sigmoid':\n",
    "            return 1 / (1 + np.exp(-x))\n",
    "        elif self.type == 'tanh':\n",
    "            return np.tanh(x)\n",
    "        elif self.type == 'softmax':\n",
    "            e_x = np.exp(x - np.max(x, axis=0, keepdims=True))\n",
    "            return e_x / np.sum(e_x, axis=0, keepdims=True)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported activation type: {self.type}\")\n",
    "\n",
    "    def derivative(self, x):\n",
    "        if self.type == 'linear':\n",
    "            return np.ones_like(x)\n",
    "        elif self.type == 'relu':\n",
    "            return np.where(x > 0, 1, 0)\n",
    "        elif self.type == 'sigmoid':\n",
    "            s = self.forward(x)\n",
    "            return s * (1 - s)\n",
    "        elif self.type == 'tanh':\n",
    "            return 1 - np.tanh(x) ** 2\n",
    "        else:\n",
    "            raise NotImplementedError(f\"Derivative not implemented or not required for: {self.type}\")\n",
    "\n",
    "class Parameters:\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        self.weights = np.random.randn(output_dim, input_dim) * 0.01\n",
    "        self.bias = np.zeros((output_dim, 1))\n",
    "\n",
    "class Layer:\n",
    "    def __init__(self, input_dim, output_dim, activation_type):\n",
    "        self.params = Parameters(input_dim, output_dim)\n",
    "        self.activation = Activation(activation_type)\n",
    "\n",
    "    def forward(self, a_prev):\n",
    "        z = np.dot(self.params.weights, a_prev) + self.params.bias\n",
    "        return self.activation.forward(z), z\n",
    "\n",
    "class DeepNeuralNetwork:\n",
    "    def __init__(self, layer_dims, activation_types):\n",
    "        self.layers = []\n",
    "        self.L = len(layer_dims) - 1  # number of layers\n",
    "        for i in range(1, len(layer_dims)):\n",
    "            self.layers.append(Layer(layer_dims[i-1], layer_dims[i], activation_types[i-1]))\n",
    "\n",
    "    def forward_propagation(self, X):\n",
    "        cache = {\"A0\": X}\n",
    "        A = X\n",
    "        for i, layer in enumerate(self.layers, 1):\n",
    "            A, Z = layer.forward(A)\n",
    "            cache[f\"A{i}\"] = A\n",
    "            cache[f\"Z{i}\"] = Z\n",
    "        return A, cache\n",
    "\n",
    "    def compute_cost(self, AL, Y):\n",
    "        m = Y.shape[1]\n",
    "        cost = -np.sum(Y * np.log(AL + 1e-8)) / m\n",
    "        return cost\n",
    "\n",
    "    def backward_propagation(self, Y, cache):\n",
    "        grads = {}\n",
    "        m = Y.shape[1]\n",
    "        AL = cache[f\"A{self.L}\"]\n",
    "        Y = Y.reshape(AL.shape)  # Ensure Y is the same shape as AL\n",
    "\n",
    "    # Initialize backpropagation for the last layer differently if using softmax\n",
    "        if self.layers[-1].activation.type == 'softmax':\n",
    "            dAL = AL - Y  # This works directly for softmax with cross-entropy\n",
    "        else:\n",
    "            dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
    "\n",
    "        for l in reversed(range(self.L)):\n",
    "            current_layer = self.layers[l]\n",
    "            prev_A = cache[f\"A{l}\"]\n",
    "            Z = cache[f\"Z{l+1}\"]\n",
    "\n",
    "        # Calculating gradients\n",
    "            if l == self.L - 1:\n",
    "                dZ = dAL\n",
    "            else:\n",
    "                next_layer_W = self.layers[l + 1].params.weights\n",
    "                next_layer_dZ = grads[f\"dZ{l+2}\"]\n",
    "                dZ = np.dot(next_layer_W.T, next_layer_dZ) * current_layer.activation.derivative(Z)\n",
    "\n",
    "            dW = np.dot(dZ, prev_A.T) / m\n",
    "            db = np.sum(dZ, axis=1, keepdims=True) / m\n",
    "            dA_prev = np.dot(current_layer.params.weights.T, dZ)\n",
    "\n",
    "        # Saving gradients\n",
    "            grads[f\"dW{l+1}\"] = dW\n",
    "            grads[f\"db{l+1}\"] = db\n",
    "            grads[f\"dZ{l+1}\"] = dZ  # Saving dZ for calculation in previous layer\n",
    "\n",
    "        return grads\n",
    "\n",
    "    def update_parameters(self, grads, learning_rate):\n",
    "        # Update the network's weights and biases based on the calculated gradients and the learning rate.\n",
    "        for l in range(self.L):\n",
    "            # Loop through each layer of the network.\n",
    "\n",
    "            # Update the weights for layer l by subtracting the product of the learning rate and the gradient of the weights.\n",
    "            self.layers[l].params.weights -= learning_rate * grads[f\"dW{l+1}\"]\n",
    "\n",
    "            # Update the bias for layer l by subtracting the product of the learning rate and the gradient of the bias.\n",
    "            self.layers[l].params.bias -= learning_rate * grads[f\"db{l+1}\"]\n",
    "\n",
    "    def train(self, X, Y, epochs, learning_rate):\n",
    "        # Train the neural network using the provided training data.\n",
    "        # X: Input data, a numpy array of shape (number of features, number of examples).\n",
    "        # Y: True labels, a numpy array of shape (1, number of examples) for binary classification.\n",
    "        # epochs: Number of iterations to run the training process.\n",
    "        # learning_rate: Step size at each iteration while moving toward a minimum of the cost function.\n",
    "\n",
    "        for i in range(epochs):\n",
    "            # Loop through the specified number of epochs to perform training.\n",
    "\n",
    "            # Perform forward propagation to compute the network's output AL and cache intermediate values.\n",
    "            AL, cache = self.forward_propagation(X)\n",
    "\n",
    "            # Compute the cost (loss) using the network's output AL and the true labels Y.\n",
    "            cost = self.compute_cost(AL, Y)\n",
    "\n",
    "            # Perform backward propagation to compute the gradients of the cost with respect to the parameters.\n",
    "            grads = self.backward_propagation(Y, cache)\n",
    "\n",
    "            # Update the network's parameters (weights and biases) using the computed gradients.\n",
    "            self.update_parameters(grads, learning_rate)\n",
    "\n",
    "            # Optionally, print the cost every 100 epochs to monitor the training progress.\n",
    "            if i % 100 == 0:\n",
    "                print(f\"Loss (Cost) after iteration {i}: {cost}\")\n",
    "\n",
    "# Simulating a dataset for training\n",
    "m = 1000  # Number of examples\n",
    "input_dim = 784  # Example input size (e.g., a flattened 28x28 grayscale image)\n",
    "output_dim = 1  # Output size for binary classification\n",
    "X = np.random.randn(input_dim, m)  # Simulating input features with random values\n",
    "Y = np.random.randint(0, 2, (output_dim, m))  # Simulating binary labels (0 or 1)\n",
    "\n",
    "# Network configuration\n",
    "layer_dims = [784, 128, 64, 1]  # Example architecture: input layer, two hidden layers, and output layer\n",
    "activation_types = ['relu', 'relu', 'sigmoid']  # Using ReLU for hidden layers and sigmoid for the output layer\n",
    "\n",
    "# Initialize the network\n",
    "network = DeepNeuralNetwork(layer_dims, activation_types)\n",
    "\n",
    "# Train the network\n",
    "epochs = 2000\n",
    "learning_rate = 0.01\n",
    "network.train(X, Y, epochs, learning_rate)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
