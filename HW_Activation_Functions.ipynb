{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss (Cost) after iteration 0: 0.33697825686595484\n",
      "Loss (Cost) after iteration 100: 0.34563715756580515\n",
      "Loss (Cost) after iteration 200: 0.3487991281537742\n",
      "Loss (Cost) after iteration 300: 0.34994046986925637\n",
      "Loss (Cost) after iteration 400: 0.35033837009809127\n",
      "Loss (Cost) after iteration 500: 0.3504464553051579\n",
      "Loss (Cost) after iteration 600: 0.35043806965701635\n",
      "Loss (Cost) after iteration 700: 0.35034123094199654\n",
      "Loss (Cost) after iteration 800: 0.3501569402715359\n",
      "Loss (Cost) after iteration 900: 0.34979467137769993\n",
      "Loss (Cost) after iteration 1000: 0.349033024652897\n",
      "Loss (Cost) after iteration 1100: 0.3468630997512577\n",
      "Loss (Cost) after iteration 1200: 0.33844624559249215\n",
      "Loss (Cost) after iteration 1300: 0.28993378867921216\n",
      "Loss (Cost) after iteration 1400: 0.06751102251317229\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/36/chw789xx1jv9xm2cl1v87_4c0000gn/T/ipykernel_26311/1382670908.py:159: RuntimeWarning: invalid value encountered in divide\n",
      "  dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss (Cost) after iteration 1500: nan\n",
      "Loss (Cost) after iteration 1600: nan\n",
      "Loss (Cost) after iteration 1700: nan\n",
      "Loss (Cost) after iteration 1800: nan\n",
      "Loss (Cost) after iteration 1900: nan\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Activation:\n",
    "    def __init__(self, activation_type):\n",
    "        # Initialize the activation object with a specified type of activation function.\n",
    "        self.type = activation_type\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Compute the forward pass output using the specified activation function.\n",
    "        if self.type == 'linear':\n",
    "            # Linear activation does nothing and returns the input as is.\n",
    "            return x\n",
    "        elif self.type == 'relu':\n",
    "            # ReLU (Rectified Linear Unit) activation returns the input if it's positive, otherwise 0.\n",
    "            return np.maximum(0, x)\n",
    "        elif self.type == 'sigmoid':\n",
    "            # Sigmoid activation squashes the input to a range between 0 and 1.\n",
    "            return 1 / (1 + np.exp(-x))\n",
    "        elif self.type == 'tanh':\n",
    "            # Tanh (Hyperbolic Tangent) activation squashes the input to a range between -1 and 1.\n",
    "            return np.tanh(x)\n",
    "        elif self.type == 'softmax':\n",
    "            # Softmax activation computes the exponential of each input, then normalizes by dividing by the sum of all exponentials.\n",
    "            e_x = np.exp(x - np.max(x, axis=0, keepdims=True))  # Subtract max for numerical stability.\n",
    "            return e_x / np.sum(e_x, axis=0, keepdims=True)\n",
    "        else:\n",
    "            # If an unsupported activation type is specified, raise an error.\n",
    "            raise ValueError(f\"Unsupported activation type: {self.type}\")\n",
    "\n",
    "    def derivative(self, x):\n",
    "        # Compute the derivative of the activation function for backpropagation.\n",
    "        if self.type == 'linear':\n",
    "            # The derivative of the linear function is 1.\n",
    "            return np.ones_like(x)\n",
    "        elif self.type == 'relu':\n",
    "            # The derivative of ReLU is 1 for positive inputs and 0 otherwise.\n",
    "            return np.where(x > 0, 1, 0)\n",
    "        elif self.type == 'sigmoid':\n",
    "            # The derivative of the sigmoid function can be expressed in terms of its output.\n",
    "            s = self.forward(x)\n",
    "            return s * (1 - s)\n",
    "        elif self.type == 'tanh':\n",
    "            # The derivative of the tanh function can be expressed in terms of its output as well.\n",
    "            return 1 - np.tanh(x) ** 2\n",
    "        else:\n",
    "            # If the derivative is called for an unsupported or not required activation function, raise an error.\n",
    "            raise NotImplementedError(f\"Derivative not implemented or not required for: {self.type}\")\n",
    "\n",
    "class Parameters:\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        # Constructor for the Parameters class to initialize weights and biases.\n",
    "        \n",
    "        # input_dim: The size of the input layer or the number of features.\n",
    "        # output_dim: The size of the output from this layer or the number of neurons in the layer.\n",
    "        \n",
    "        # Initialize the weights matrix with small random values.\n",
    "        # The shape of the weights matrix is (output_dim, input_dim) to match the matrix multiplication\n",
    "        # requirements during the forward pass: (output_dim x input_dim) dot (input_dim x m) = (output_dim x m),\n",
    "        # where m is the number of examples. Multiplying by 0.01 keeps the initial weights small, aiding in\n",
    "        # maintaining the stability of the learning process by preventing saturation of neurons.\n",
    "        self.weights = np.random.randn(output_dim, input_dim) * 0.01\n",
    "        \n",
    "        # Initialize the bias vector with zeros.\n",
    "        # The shape of the bias vector is (output_dim, 1) to ensure it can be added directly to the weighted inputs\n",
    "        # for each neuron in the layer. Each neuron in the layer has a single bias term.\n",
    "        self.bias = np.zeros((output_dim, 1))\n",
    "\n",
    "class Layer:\n",
    "    def __init__(self, input_dim, output_dim, activation_type):\n",
    "        # Constructor for the Layer class to initialize a layer in the neural network.\n",
    "        \n",
    "        # input_dim: The size of the input for this layer, or the number of features/units in the previous layer.\n",
    "        # output_dim: The number of neurons in this layer, determining the size of the output from this layer.\n",
    "        # activation_type: A string specifying the type of activation function to be used in this layer.\n",
    "        \n",
    "        # Initialize the parameters of the layer (weights and biases) using the Parameters class.\n",
    "        # This encapsulation simplifies the management of the layer's parameters.\n",
    "        self.params = Parameters(input_dim, output_dim)\n",
    "        \n",
    "        # Initialize the activation function for the layer based on the specified type.\n",
    "        # The Activation class encapsulates various activation functions, facilitating easy switching\n",
    "        # and experimentation with different activation types.\n",
    "        self.activation = Activation(activation_type)\n",
    "\n",
    "    def forward(self, a_prev):\n",
    "        # Compute the forward pass for this layer.\n",
    "        \n",
    "        # a_prev: The activations from the previous layer (or the input data for the first layer).\n",
    "        # This is a matrix with the shape (input_dim, m), where m is the number of examples.\n",
    "        \n",
    "        # Compute the linear part of the layer's forward pass: Z = W.X + b, where\n",
    "        # W is the weight matrix, X is the input, and b is the bias vector.\n",
    "        # np.dot(self.params.weights, a_prev) performs the matrix multiplication W.X,\n",
    "        # and adding self.params.bias broadcasts the bias to each column of the result.\n",
    "        z = np.dot(self.params.weights, a_prev) + self.params.bias\n",
    "        \n",
    "        # Apply the activation function to the linear combination computed above.\n",
    "        # This introduces non-linearity to the layer's output, allowing the network\n",
    "        # to learn more complex functions. The activation function's output is the\n",
    "        # final output (A) of this layer. The method returns both A and Z for use\n",
    "        # in subsequent calculations, particularly during backpropagation.\n",
    "        return self.activation.forward(z), z\n",
    "\n",
    "class DeepNeuralNetwork:\n",
    "    def __init__(self, layer_dims, activation_types):\n",
    "        # Constructor for initializing a deep neural network.\n",
    "        # layer_dims: List of integers, where each integer represents the number of neurons in each layer.\n",
    "        # activation_types: List of strings, specifying the activation function for each layer.\n",
    "\n",
    "        self.layers = []  # List to store each layer of the network.\n",
    "        self.L = len(layer_dims) - 1  # Total number of layers in the network excluding the input layer.\n",
    "\n",
    "        # Loop through the layer dimensions and create each layer with the appropriate dimensions\n",
    "        # and activation function.\n",
    "        for i in range(1, len(layer_dims)):\n",
    "            self.layers.append(Layer(layer_dims[i-1], layer_dims[i], activation_types[i-1]))\n",
    "\n",
    "    def forward_propagation(self, X):\n",
    "        # Perform forward propagation through the network.\n",
    "        # X: Input data, a numpy array of shape (number of features, number of examples).\n",
    "\n",
    "        cache = {\"A0\": X}  # Initialize cache to store activations (A) and linear combinations (Z) for each layer.\n",
    "        A = X  # Set the initial activation to the input data.\n",
    "\n",
    "        # Loop through each layer and perform forward propagation, updating the cache with\n",
    "        # activations and linear combinations for each layer.\n",
    "        for i, layer in enumerate(self.layers, 1):\n",
    "            A, Z = layer.forward(A)  # Get the activation and linear combination from the current layer.\n",
    "            cache[f\"A{i}\"] = A  # Store the activation for the current layer in the cache.\n",
    "            cache[f\"Z{i}\"] = Z  # Store the linear combination for the current layer in the cache.\n",
    "\n",
    "        return A, cache  # Return the final activation output and the cache.\n",
    "\n",
    "    def compute_cost(self, AL, Y):\n",
    "        # Compute the cost (loss) using the output of the network and the true labels.\n",
    "        # AL: Final layer activations, the predictions of the network.\n",
    "        # Y: True labels.\n",
    "\n",
    "        m = Y.shape[1]  # Number of examples.\n",
    "        # Compute the cross-entropy cost.\n",
    "        cost = -np.sum(Y * np.log(AL + 1e-8)) / m\n",
    "        return cost\n",
    "\n",
    "    def backward_propagation(self, Y, cache):\n",
    "        # Perform backward propagation to compute gradients for learning.\n",
    "        # Y: True labels.\n",
    "        # cache: Cache containing the activations and linear combinations from forward propagation.\n",
    "\n",
    "        grads = {}  # Dictionary to store gradients.\n",
    "        m = Y.shape[1]  # Number of examples.\n",
    "        AL = cache[f\"A{self.L}\"]  # Activation of the last layer.\n",
    "        Y = Y.reshape(AL.shape)  # Ensure Y has the same shape as AL for operations.\n",
    "\n",
    "        # Initialize backpropagation. The gradient of the cost with respect to AL differs\n",
    "        # based on the activation type of the last layer.\n",
    "        if self.layers[-1].activation.type == 'softmax':\n",
    "            dAL = AL - Y  # Gradient for softmax with cross-entropy loss.\n",
    "        else:\n",
    "            dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
    "\n",
    "        # Loop through layers in reverse order to perform backpropagation.\n",
    "        for l in reversed(range(self.L)):\n",
    "            current_layer = self.layers[l]\n",
    "            prev_A = cache[f\"A{l}\"]\n",
    "            Z = cache[f\"Z{l+1}\"]\n",
    "\n",
    "            # Calculate gradients for the current layer.\n",
    "            if l == self.L - 1:\n",
    "                dZ = dAL\n",
    "            else:\n",
    "                next_layer_W = self.layers[l + 1].params.weights\n",
    "                next_layer_dZ = grads[f\"dZ{l+2}\"]\n",
    "                dZ = np.dot(next_layer_W.T, next_layer_dZ) * current_layer.activation.derivative(Z)\n",
    "\n",
    "            dW = np.dot(dZ, prev_A.T) / m\n",
    "            db = np.sum(dZ, axis=1, keepdims=True) / m\n",
    "            if l > 0:\n",
    "                dA_prev = np.dot(current_layer.params.weights.T, dZ)\n",
    "\n",
    "            # Store gradients for the current layer.\n",
    "            grads[f\"dW{l+1}\"] = dW\n",
    "            grads[f\"db{l+1}\"] = db\n",
    "            if l > 0:  # dZ is not needed for the input layer\n",
    "                grads[f\"dZ{l+1}\"] = dZ\n",
    "\n",
    "        return grads\n",
    "\n",
    "    def update_parameters(self, grads, learning_rate):\n",
    "        # Update the network's weights and biases based on the calculated gradients and the learning rate.\n",
    "        for l in range(self.L):\n",
    "            # Loop through each layer of the network.\n",
    "\n",
    "            # Update the weights for layer l by subtracting the product of the learning rate and the gradient of the weights.\n",
    "            self.layers[l].params.weights -= learning_rate * grads[f\"dW{l+1}\"]\n",
    "\n",
    "            # Update the bias for layer l by subtracting the product of the learning rate and the gradient of the bias.\n",
    "            self.layers[l].params.bias -= learning_rate * grads[f\"db{l+1}\"]\n",
    "\n",
    "    def train(self, X, Y, epochs, learning_rate):\n",
    "        # Train the neural network using the provided training data.\n",
    "        # X: Input data, a numpy array of shape (number of features, number of examples).\n",
    "        # Y: True labels, a numpy array of shape (1, number of examples) for binary classification.\n",
    "        # epochs: Number of iterations to run the training process.\n",
    "        # learning_rate: Step size at each iteration while moving toward a minimum of the cost function.\n",
    "\n",
    "        for i in range(epochs):\n",
    "            # Loop through the specified number of epochs to perform training.\n",
    "\n",
    "            # Perform forward propagation to compute the network's output AL and cache intermediate values.\n",
    "            AL, cache = self.forward_propagation(X)\n",
    "\n",
    "            # Compute the cost (loss) using the network's output AL and the true labels Y.\n",
    "            cost = self.compute_cost(AL, Y)\n",
    "\n",
    "            # Perform backward propagation to compute the gradients of the cost with respect to the parameters.\n",
    "            grads = self.backward_propagation(Y, cache)\n",
    "\n",
    "            # Update the network's parameters (weights and biases) using the computed gradients.\n",
    "            self.update_parameters(grads, learning_rate)\n",
    "\n",
    "            # Optionally, print the cost every 100 epochs to monitor the training progress.\n",
    "            if i % 100 == 0:\n",
    "                print(f\"Loss (Cost) after iteration {i}: {cost}\")\n",
    "           \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Simulating a dataset for training\n",
    "m = 1000  # Number of examples\n",
    "\n",
    "input_dim = 784  # Example input size (e.g., a flattened 28x28 grayscale image)\n",
    "output_dim = 1  # Output size for binary classification\n",
    "X = np.random.randn(input_dim, m)  # Simulating input features with random values\n",
    "Y = np.random.randint(0, 2, (output_dim, m))  # Simulating binary labels (0 or 1)\n",
    "\n",
    "# Network configuration\n",
    "layer_dims = [784, 128, 64, 1]  # Example architecture: input layer, two hidden layers, and output layer\n",
    "activation_types = ['relu', 'relu', 'sigmoid']  # Using ReLU for hidden layers and sigmoid for the output layer\n",
    "\n",
    "# Initialize the network\n",
    "network = DeepNeuralNetwork(layer_dims, activation_types)\n",
    "\n",
    "# Train the network\n",
    "epochs = 2000\n",
    "learning_rate = 0.01\n",
    "network.train(X, Y, epochs, learning_rate)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
